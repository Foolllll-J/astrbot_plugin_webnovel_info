import asyncio
import aiohttp
import base64
import re
import os
from yarl import URL
from cachetools import TTLCache
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register, StarTools
from astrbot.api import logger
import astrbot.api.message_components as Comp

from .sources import SourceManager
from .core.search_engine import MultiSearchEngine
from .core.bookshelf_manager import BookshelfManager

@register("astrbot_plugin_webnovel_info", "Foolllll", "ç½‘æ–‡æœç´¢åŠ©æ‰‹", "1.0", "")
class WebnovelInfoPlugin(Star):
    """ç½‘æ–‡æœç´¢æ’ä»¶æ ¸å¿ƒç±»
    æ”¯æŒå¤šå¹³å°ä¹¦ç±æœç´¢ã€åˆ†é¡µã€è¯¦æƒ…æŸ¥çœ‹ã€è¯•è¯»å†…å®¹å±•ç¤º
    """
    def __init__(self, context: Context, config=None):
        super().__init__(context)
        self.source_manager = SourceManager()  # æ•°æ®æºç®¡ç†å™¨
        self.bookshelf_manager = BookshelfManager(StarTools.get_data_dir("astrbot_plugin_webnovel_info"))
        self.config = config or {}             # æ’ä»¶é…ç½®ï¼ˆé»˜è®¤ç©ºå­—å…¸ï¼‰
        
        # æ˜¾ç¤ºæ¨¡å¼ï¼šç®€æ´/è¯¦ç»†ï¼ˆé»˜è®¤è¯¦ç»†ï¼‰
        self.display_mode = "concise" if self.config.get("display_mode", "è¯¦ç»†") == "ç®€æ´" else "detailed"
        self.enable_trial = self.config.get("enable_trial", False)  # æ˜¯å¦å¯ç”¨è¯•è¯»åŠŸèƒ½
        self.priority_cfg = self.config.get("platform_weights", "1 2 2").split()  # å¹³å°æƒé‡é…ç½®
        
        # åˆå§‹åŒ–ç•ªèŒ„ API é…ç½®
        if "tomato" in self.source_manager.sources:
            self.source_manager.get_source("tomato").api_base = self.config.get("tomato_api_base", "")

        self.user_search_state = TTLCache(maxsize=1000, ttl=3600)
        
        self.trial_content_limit = 3000  # è¯•è¯»å†…å®¹é•¿åº¦é™åˆ¶ï¼ˆå­—ç¬¦æ•°ï¼‰
        self.page_size = 10  
        self._session = None # æŒä¹…åŒ–ä¼šè¯

    async def get_session(self):
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession(headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            })
        return self._session

    def _get_user_search_state(self, user_id: str):
        """è·å–/åˆå§‹åŒ–ç”¨æˆ·æœç´¢çŠ¶æ€
        
        Args:
            user_id: ç”¨æˆ·å”¯ä¸€æ ‡è¯†
        
        Returns:
            dict: ç”¨æˆ·æœç´¢çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«å…³é”®è¯ã€é¡µç ã€ç¼“å­˜ç­‰ä¿¡æ¯
        """
        if user_id not in self.user_search_state:
            self.user_search_state[user_id] = {
                "keyword": "",          # æœç´¢å…³é”®è¯
                "current_page": 1,      # å½“å‰é¡µç 
                "full_pool": [],        # å¤šå¹³å°ç»¼åˆç»“æœæ± 
                "raw_pool": [],         # åŸå§‹æœç´¢ç»“æœæ± 
                "qd_page": 1,           # èµ·ç‚¹æœç´¢é¡µç 
                "cwm_page": 1,          # åˆºçŒ¬çŒ«æœç´¢é¡µç 
                "tm_page": 1,           # ç•ªèŒ„æœç´¢é¡µç 
                "qd_last": False,       # èµ·ç‚¹æ˜¯å¦æœ€åä¸€é¡µ
                "cwm_last": False,      # åˆºçŒ¬çŒ«æ˜¯å¦æœ€åä¸€é¡µ
                "tm_last": False,       # ç•ªèŒ„æ˜¯å¦æœ€åä¸€é¡µ
                "source": "",           # å½“å‰æœç´¢æºï¼ˆmulti/qidian/ciweimao/tomatoï¼‰
                "max_pages": 1,         # æ€»é¡µæ•°
                "results": [],          # å½“å‰é¡µç»“æœ
                "single_pool": [],      # å•å¹³å°ç»“æœæ± 
                "cached_pages": {},     # é¡µç ç¼“å­˜ï¼ˆkey:é¡µç ï¼Œvalue:è¯¥é¡µæ•°æ®ï¼‰
                "last_viewed": None,    # æœ€è¿‘æŸ¥çœ‹çš„ä¹¦ç±ä¿¡æ¯
                "bookshelf_page": 1     # ä¹¦æ¶å½“å‰é¡µç 
            }
        return self.user_search_state[user_id]

    @filter.command("æœä¹¦", alias={'ss'})
    async def multi_search_handler(self, event: AstrMessageEvent):
        """å¤šå¹³å°ç»¼åˆæœç´¢å¤„ç†å‡½æ•°
        æ”¯æŒæŒ‡ä»¤ï¼š/ss <ä¹¦å> | /ss <åºå·> | /ss ä¸Šä¸€é¡µ/ä¸‹ä¸€é¡µ
        """
        parts = event.message_str.strip().split()
        if len(parts) < 2:
            yield event.plain_result("ç”¨æ³•: /ss <ä¹¦å> æˆ– /ss <åºå·> æˆ– /ss ä¸‹ä¸€é¡µ")
            return

        # è§£æç”¨æˆ·IDå’Œæ“ä½œæŒ‡ä»¤
        user_id, action = event.get_sender_id(), parts[1]
        state = self._get_user_search_state(user_id)
        avg_threshold = 60  # ç»“æœç­›é€‰é˜ˆå€¼
        direct_index = None

        # 1. åºå·æŸ¥è¯¢ï¼šæŸ¥çœ‹æŒ‡å®šä¹¦ç±è¯¦æƒ… (e.g. /ss 1)
        if action.isdigit() and len(parts) == 2:
            idx = int(action) - 1
            if 0 <= idx < len(state["full_pool"]):
                target = state["full_pool"][idx]
                state["last_viewed"] = target # è®°å½•æœ€è¿‘æŸ¥çœ‹
                details = await self.source_manager.get_source(target['origin']).get_book_details(target["url"])
                if details:
                    yield event.chain_result(await self._format_book_details(details))
                return
            yield event.plain_result(f"ğŸ¤” åºå· {action} ä¸åœ¨å½“å‰ç»“æœä¸­ã€‚")
            return

        # 2. ç¿»é¡µæ“ä½œ (e.g. /ss ä¸‹ä¸€é¡µ)
        if action in ["ä¸‹ä¸€é¡µ", "ä¸‹é¡µ", "ä¸Šä¸€é¡µ", "ä¸Šé¡µ"] and len(parts) == 2:
            if not state["keyword"]:
                yield event.plain_result("âŒ è¯·å…ˆæœç´¢ã€‚")
                return
            req_page = state["current_page"] + (1 if action in ["ä¸‹ä¸€é¡µ", "ä¸‹é¡µ"] else -1)
            if req_page < 1:
                yield event.plain_result("â¬…ï¸ å·²ç»æ˜¯ç¬¬ä¸€é¡µã€‚")
                return
            keyword = state["keyword"]
        # 3. æ–°å…³é”®è¯æœç´¢ æˆ– ç›´æ¥æŸ¥çœ‹è¯¦æƒ… (e.g. /ss è¯¡ç§˜ä¹‹ä¸» æˆ– /ss è¯¡ç§˜ä¹‹ä¸» 1)
        else:
            # è§£æç›´æ¥æŸ¥çœ‹è¯¦æƒ…ç´¢å¼•
            if len(parts) >= 3 and parts[-1].isdigit():
                try:
                    direct_index = int(parts[-1])
                    keyword = " ".join(parts[1:-1])
                except ValueError:
                    keyword = " ".join(parts[1:])
            else:
                keyword = " ".join(parts[1:])
            
            if not keyword:
                yield event.plain_result("âŒ è¯·è¾“å…¥å…³é”®è¯ï¼Œä¾‹å¦‚ï¼š`/ss è¯¡ç§˜ä¹‹ä¸»`")
                return
            
            req_page = 1
            if state["keyword"] != keyword:
                yield event.plain_result(f"ğŸ” æ­£åœ¨å¤šå¹³å°æœç´¢â€œ{keyword}â€...")
                # é‡ç½®æœç´¢çŠ¶æ€
                state.update({
                    "keyword": keyword, "full_pool": [], "raw_pool": [], 
                    "qd_page": 1, "cwm_page": 1, "tm_page": 1,
                    "qd_last": False, "cwm_last": False, "tm_last": False,
                    "source": "multi",
                    "cached_pages": {}
                })

        # è®¡ç®—ç›®æ ‡é¡µæ•°éœ€è¦çš„ç»“æœæ€»æ•°
        if direct_index:
            target_count = direct_index
        else:
            target_count = req_page * self.page_size
        qd_prio = self.priority_cfg[0] if len(self.priority_cfg) > 0 else "1"
        tm_prio = self.priority_cfg[1] if len(self.priority_cfg) > 1 else "2"
        cwm_prio = self.priority_cfg[2] if len(self.priority_cfg) > 2 else "2"
        
        weights_map = {
            "qidian": MultiSearchEngine.get_weight(qd_prio), 
            "tomato": MultiSearchEngine.get_weight(tm_prio),
            "ciweimao": MultiSearchEngine.get_weight(cwm_prio)
        }

        # è¡¥å……ç»“æœæ± ç›´åˆ°æ»¡è¶³ç›®æ ‡é¡µæ•°éœ€æ±‚
        avg_threshold = 60  # ç»“æœç­›é€‰é˜ˆå€¼
        max_batches = 5     # æœ€å¤§æ‹‰å–æ‰¹æ¬¡ï¼Œé˜²æ­¢ä½è´¨é‡ç»“æœå¯¼è‡´æ— é™æ‹‰å–
        batch_count = 0
        
        while len(state["full_pool"]) < target_count and batch_count < max_batches:
            batch_count += 1
            _, _, current_avg = MultiSearchEngine.sift_by_average(state["raw_pool"], keyword, weights_map)
            
            # ç»“æœä¸è¶³æˆ–è´¨é‡ä¸è¾¾æ ‡æ—¶ï¼Œæ‹‰å–æ›´å¤šæ•°æ®
            # å¦‚æœæ‰€æœ‰å¹³å°éƒ½å·²æ‹‰å®Œï¼Œæˆ–è€…å½“å‰å·²ç»æœ‰è¶³å¤Ÿå¤šçš„åŸå§‹ç»“æœä½†è´¨é‡ä»ä¸è¾¾æ ‡ï¼Œåˆ™åœæ­¢æ‹‰å–
            all_exhausted = state["qd_last"] and state["cwm_last"] and state["tm_last"]
            need_more = not state["raw_pool"] or (current_avg < avg_threshold and not all_exhausted)
            
            if need_more:
                tasks, p_map = [], []
                # èµ·ç‚¹æœç´¢ä»»åŠ¡
                if qd_prio != "0" and not state["qd_last"]:
                    tasks.append(self.source_manager.get_source("qidian").search_book(keyword, page=state["qd_page"], return_metadata=True))
                    p_map.append("qidian")
                # åˆºçŒ¬çŒ«æœç´¢ä»»åŠ¡
                if cwm_prio != "0" and not state["cwm_last"]:
                    tasks.append(self.source_manager.get_source("ciweimao").search_book(keyword, page=state["cwm_page"], return_metadata=True))
                    p_map.append("ciweimao")
                # ç•ªèŒ„æœç´¢ä»»åŠ¡
                if tm_prio != "0" and not state["tm_last"] and self.config.get("tomato_api_base"):
                    tasks.append(self.source_manager.get_source("tomato").search_book(keyword, page=state["tm_page"], return_metadata=True))
                    p_map.append("tomato")
                
                if tasks:
                    # å¹¶å‘æ‰§è¡Œæœç´¢ä»»åŠ¡
                    logger.debug(f"[èšåˆæœç´¢] æ­£åœ¨æ‰§è¡Œç¬¬ {batch_count} æ‰¹æ¬¡æ‹‰å–, å…³é”®è¯: {keyword}")
                    results = await asyncio.gather(*tasks)
                    for i, r in enumerate(results):
                        if not r: continue
                        books = r.get('books', [])
                        platform = p_map[i]
                        if platform == "qidian":
                            state["qd_page"] += 1
                            state["qd_last"] = r.get('is_last', False)
                        elif platform == "ciweimao":
                            state["cwm_page"] += 1
                            state["cwm_last"] = r.get('is_last', False)
                        elif platform == "tomato":
                            state["tm_page"] += 1
                            state["tm_last"] = r.get('is_last', False)
                        state["raw_pool"].extend(books)
                    
                    # æ‹‰å–åé‡æ–°è®¡ç®—è¯„åˆ†ï¼Œå¦‚æœè¿˜æ˜¯æ²¡ç»“æœä¸”æ²¡åˆ°é™åˆ¶ï¼Œç»§ç»­å¾ªç¯æ‹‰å–
                    _, _, current_avg = MultiSearchEngine.sift_by_average(state["raw_pool"], keyword, weights_map)
                    if not state["raw_pool"] and not all_exhausted:
                        continue
            
            # ç­›é€‰é«˜è´¨é‡ç»“æœå¹¶äº¤å‰æ’åº
            # æ³¨æ„ï¼šå³ä½¿ current_avg < avg_thresholdï¼Œåªè¦æ± å­é‡Œæœ‰ä¸œè¥¿ï¼Œæˆ‘ä»¬ä¹Ÿè¿›è¡Œä¸€æ¬¡ç­›é€‰
            # è¿™æ ·å¯ä»¥ä¿è¯å³ä½¿æ²¡æœ‰å®Œç¾åŒ¹é…ï¼Œä¹Ÿèƒ½å±•ç¤ºå½“å‰æœ€æ¥è¿‘çš„ç»“æœ
            if state["raw_pool"]:
                good_batch, remains, _ = MultiSearchEngine.sift_by_average(state["raw_pool"], keyword, weights_map)
                if good_batch:
                    interleaved = MultiSearchEngine.interleave_results(good_batch, qd_prio, tm_prio, cwm_prio)
                    state["full_pool"].extend(interleaved)
                    state["raw_pool"] = remains
                else:
                    # å¦‚æœè¿™ä¸€æ‰¹æ¬¡æ²¡æœ‰â€œé«˜äºå¹³å‡åˆ†â€çš„ç»“æœï¼ˆç†è®ºä¸Šä¸å¯èƒ½ï¼Œé™¤éå…¨0åˆ†ï¼‰
                    # åˆ™æŠŠ raw_pool çš„å†…å®¹å¼ºè¡ŒæŒ‰åˆ†æ•°æ’åºæ”¾å…¥ full_pool
                    if all_exhausted or batch_count >= max_batches:
                        sorted_raw = sorted(state["raw_pool"], key=lambda x: x.get('final_score', 0), reverse=True)
                        state["full_pool"].extend(sorted_raw)
                        state["raw_pool"] = []
            elif all_exhausted:
                break

        # å¦‚æœæ˜¯ç›´æ¥æŸ¥çœ‹è¯¦æƒ…æ¨¡å¼
        if direct_index is not None:
            if 1 <= direct_index <= len(state["full_pool"]):
                target = state["full_pool"][direct_index - 1]
                state["last_viewed"] = target # è®°å½•æœ€è¿‘æŸ¥çœ‹
                details = await self.source_manager.get_source(target['origin']).get_book_details(target["url"])
                if details:
                    yield event.chain_result(await self._format_book_details(details))
                return
            else:
                yield event.plain_result(f"âš ï¸ åºå· {direct_index} è¶…å‡ºç»¼åˆæœç´¢ç»“æœèŒƒå›´ï¼ˆå…± {len(state['full_pool'])} æ¡ï¼‰ï¼Œå°†æ˜¾ç¤ºæœç´¢åˆ—è¡¨ã€‚")

        # è®¡ç®—å½“å‰é¡µå±•ç¤ºçš„ç»“æœèŒƒå›´
        start_idx = (req_page - 1) * self.page_size
        display_list = state["full_pool"][start_idx: start_idx + self.page_size]
        state["current_page"] = req_page

        # æ— ç»“æœæç¤º
        if not display_list:
            yield event.plain_result(f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°åŒ¹é…â€œ{keyword}â€çš„é«˜è´¨é‡ç»“æœã€‚")
            return

        # 1. æ£€æŸ¥æ˜¯å¦è¿˜èƒ½æ‹‰å–æ›´å¤šæ•°æ®ï¼ˆèµ·ç‚¹/åˆºçŒ¬çŒ«/ç•ªèŒ„æœªåˆ°æœ€åä¸€é¡µï¼‰
        can_load_more = False
        if not state["qd_last"] or not state["cwm_last"] or not state["tm_last"]:
            can_load_more = True
        
        # 2. è®¡ç®—å½“å‰æ€»é¡µæ•°ï¼ˆå·²åŠ è½½æ•°æ®ï¼‰
        current_total_pages = (len(state["full_pool"]) + self.page_size - 1) // self.page_size
        # 3. åˆ¤æ–­æ˜¯å¦æœ‰ä¸‹ä¸€é¡µï¼ˆå·²åŠ è½½å¤Ÿä¸‹ä¸€é¡µ æˆ– è¿˜èƒ½åŠ è½½æ›´å¤šæ•°æ®ï¼‰
        has_next_page = False
        if req_page < current_total_pages:
            has_next_page = True
        elif can_load_more and (req_page + 1) * self.page_size > len(state["full_pool"]):
            has_next_page = True
        
        # 4. æ„å»ºæ¶ˆæ¯ï¼ˆæ˜¾ç¤ºæ€»é¡µæ•°ï¼‰
        can_load_more = not state["qd_last"] or not state["cwm_last"] or not state["tm_last"]
        current_total_pages = (len(state["full_pool"]) + self.page_size - 1) // self.page_size
        
        if can_load_more:
            msg = f"ä»¥ä¸‹æ˜¯ã€{keyword}ã€‘çš„ç¬¬ {req_page} é¡µç»¼åˆæœç´¢ç»“æœï¼š\n"  # æœ‰æ›´å¤šâ†’åªæ˜¾ç¤ºå½“å‰é¡µ
        else:
            msg = f"ä»¥ä¸‹æ˜¯ã€{keyword}ã€‘çš„ç¬¬ {req_page}/{current_total_pages} é¡µç»¼åˆæœç´¢ç»“æœï¼š\n"  # æ— æ›´å¤šâ†’æ˜¾ç¤ºæ€»é¡µæ•°
        for i, b in enumerate(display_list):
            platform_tag = "[èµ·ç‚¹]" if b.get('origin') == 'qidian' else ("[åˆºçŒ¬çŒ«]" if b.get('origin') == 'ciweimao' else "[ç•ªèŒ„]")
            msg += f"{start_idx + i + 1}. {b['name']}\n    {platform_tag} ä½œè€…ï¼š{b['author']}\n"
        
        # 5. æ„å»ºç¿»é¡µæç¤º
        page_tips = []
        page_tips.append(f"/ss ä¸Šä¸€é¡µ") if req_page > 1 else None
        page_tips.append(f"/ss ä¸‹ä¸€é¡µ") if has_next_page else None
        
        logger.info(f"ç”¨æˆ· {user_id} æœç´¢ã€{keyword}ã€‘ç¬¬ {req_page} é¡µç»“æœï¼Œå½“å‰æ± ä¸­å…±æœ‰ {len(state['full_pool'])} æ¡ç»“æœï¼Œå¯åŠ è½½æ›´å¤šï¼š{can_load_more}ã€‚")
        
        # 6. è¡¥å……æ“ä½œæç¤º
        msg += f"\nğŸ’¡ `/ss <åºå·>` æŸ¥çœ‹è¯¦æƒ…\n"
        if page_tips:
            msg += f"ğŸ’¡ ä½¿ç”¨ {' | '.join(page_tips)} ç¿»é¡µ"
        else:
            if req_page == 1 and len(state["full_pool"]) <= self.page_size and not can_load_more:
                msg += "ğŸ’¡ å½“å‰å·²æ˜¯å…¨éƒ¨ç»“æœï¼Œæ— æ›´å¤šå†…å®¹"
            elif req_page > 1 and not has_next_page and not can_load_more:
                msg += "ğŸ’¡ å½“å‰å·²æ˜¯æœ€åä¸€é¡µï¼Œæ— æ›´å¤šå†…å®¹"
        
        yield event.plain_result(msg)

    @filter.command("èµ·ç‚¹", alias={'qd'})
    async def qidian_handler(self, event: AstrMessageEvent):
        """èµ·ç‚¹ä¸­æ–‡ç½‘ä¸“å±æœç´¢"""
        async for res in self._common_handler(event, "qidian", "qd", "èµ·ç‚¹"):
            yield res

    @filter.command("åˆºçŒ¬çŒ«", alias={'cwm'})
    async def ciweimao_handler(self, event: AstrMessageEvent):
        """åˆºçŒ¬çŒ«ä¸“å±æœç´¢"""
        async for res in self._common_handler(event, "ciweimao", "cwm", "åˆºçŒ¬çŒ«"):
            yield res

    @filter.command("ç•ªèŒ„", alias={'fq'})
    async def tomato_handler(self, event: AstrMessageEvent):
        """ç•ªèŒ„å°è¯´ä¸“å±æœç´¢"""
        if not self.config.get("tomato_api_base"):
            yield event.plain_result("âŒ æœªé…ç½®ç•ªèŒ„ API åŸºç¡€åœ°å€ï¼Œè¯·åœ¨é…ç½®ä¸­å¡«å†™ã€‚")
            return
        async for res in self._common_handler(event, "tomato", "fq", "ç•ªèŒ„"):
            yield res

    @filter.command("ä¸‰æ±Ÿ", alias={'sj'})
    async def sanjiang_handler(self, event: AstrMessageEvent):
        """è·å–èµ·ç‚¹ä¸‰æ±Ÿé¢‘é“æ¨èä¹¦ç±"""
        qidian = self.source_manager.get_source("qidian")
        books = await qidian.get_sanjiang_books()
        
        if not books:
            yield event.plain_result("âŒ æš‚æ—¶æ²¡æœ‰è·å–åˆ°ä¸‰æ±Ÿæ¨èä¹¦ç±ï¼Œè¯·ç¨åå†è¯•ã€‚")
            return
            
        # è®°å½•åˆ°æœç´¢çŠ¶æ€ï¼Œæ–¹ä¾¿ç”¨æˆ·ç›´æ¥é€šè¿‡åºå·çœ‹è¯¦æƒ…
        user_id = event.get_sender_id()
        state = self._get_user_search_state(user_id)
        state.update({
            "keyword": "ä¸‰æ±Ÿæ¨è",
            "source": "qidian",
            "full_pool": books, # ä¸‰æ±Ÿä¸éœ€è¦ç¿»é¡µï¼Œç›´æ¥æ”¾å…¥å…¨é‡æ± 
            "results": books,
            "cached_pages": {1: books},
            "current_page": 1,
            "max_pages": 1
        })
        
        msg = "ğŸ“– ã€èµ·ç‚¹Â·ä¸‰æ±Ÿæ¨èã€‘\n\n"
        for i, b in enumerate(books): # ä¸€æ¬¡æ€§å±•ç¤ºå…¨éƒ¨ç»“æœ
            msg += f"{i+1}. {b['name']} | {b['author']}\n"
            msg += f"   åˆ†ç±»ï¼š{b['cat']} | çŠ¶æ€ï¼š{b['state']} | {b['cnt']}\n"
            if b.get('rec'):
                msg += f"   è¯„è¯­ï¼š{b['rec']}\n"
            desc = b['desc'].replace('\r', '').replace('\n', '').strip()
            msg += f"   ç®€ä»‹ï¼š{desc[:60]}...\n\n"
            
        msg += f"ğŸ’¡ å…± {len(books)} æœ¬ã€‚ä½¿ç”¨ `/qd <åºå·>` æŸ¥çœ‹è¯¦æƒ…ã€‚"
        
        yield event.plain_result(msg.strip())



    @filter.command("æ·»åŠ ä¹¦æ¶", alias={'åŠ ä¹¦æ¶'})
    async def add_to_bookshelf(self, event: AstrMessageEvent):
        """æ·»åŠ ä¹¦ç±åˆ°ä¹¦æ¶"""
        user_id = event.get_sender_id()
        state = self._get_user_search_state(user_id)
        parts = event.message_str.strip().split()
        
        target_book = None
        
        # å¦‚æœæä¾›äº†åºå·
        if len(parts) >= 2 and parts[1].isdigit():
            idx = int(parts[1]) - 1
            if 0 <= idx < len(state.get("full_pool", [])):
                target_book = state["full_pool"][idx]
            elif 0 <= idx < len(state.get("results", [])):
                target_book = state["results"][idx]
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œä½¿ç”¨æœ€è¿‘æŸ¥çœ‹çš„ä¹¦ç±
        elif len(parts) == 1:
            target_book = state.get("last_viewed")
            
        if not target_book:
            yield event.plain_result("âŒ è¯·æä¾›æœ‰æ•ˆçš„ä¹¦ç±åºå·ï¼Œæˆ–å…ˆæŸ¥çœ‹ä¸€æœ¬ä¹¦çš„è¯¦æƒ…ã€‚")
            return
            
        success = self.bookshelf_manager.add_book(user_id, target_book)
        if success:
            yield event.plain_result(f"âœ… å·²å°†ã€Š{target_book['name']}ã€‹åŠ å…¥ä¹¦æ¶ã€‚")
        else:
            yield event.plain_result(f"ğŸ¤” ã€Š{target_book['name']}ã€‹å·²ç»åœ¨ä½ çš„ä¹¦æ¶é‡Œäº†ã€‚")

    @filter.command("ç§»é™¤ä¹¦æ¶", alias={'åˆ ä¹¦'})
    async def remove_from_bookshelf(self, event: AstrMessageEvent):
        """ä»ä¹¦æ¶ç§»é™¤ä¹¦ç±"""
        user_id = event.get_sender_id()
        state = self._get_user_search_state(user_id)
        parts = event.message_str.strip().split()
        
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œå°è¯•ç§»é™¤æœ€è¿‘æŸ¥çœ‹çš„ä¹¦ç±
        if len(parts) == 1:
            last_viewed = state.get("last_viewed")
            if last_viewed:
                success = self.bookshelf_manager.remove_book_by_info(user_id, last_viewed['bid'], last_viewed['origin'])
                if success:
                    yield event.plain_result(f"âœ… å·²å°†ã€Š{last_viewed['name']}ã€‹ä»ä¹¦æ¶ç§»é™¤ã€‚")
                    return
            yield event.plain_result("âŒ è¯·æä¾›ä¹¦æ¶ä¸­çš„ä¹¦ç±åºå·ï¼Œæˆ–å…ˆé€šè¿‡ä¹¦æ¶æŸ¥çœ‹ä¸€æœ¬ä¹¦ã€‚")
            return

        # å¦‚æœæä¾›äº†åºå·
        if parts[1].isdigit():
            idx = int(parts[1])
            removed = self.bookshelf_manager.remove_book(user_id, idx)
            if removed:
                yield event.plain_result(f"âœ… å·²å°†ã€Š{removed['name']}ã€‹ä»ä¹¦æ¶ç§»é™¤ã€‚")
            else:
                yield event.plain_result(f"âŒ ä¹¦æ¶ä¸­ä¸å­˜åœ¨åºå·ä¸º {idx} çš„ä¹¦ç±ã€‚")

    @filter.command("æŸ¥çœ‹ä¹¦æ¶", alias={'ä¹¦æ¶','æˆ‘çš„ä¹¦æ¶'})
    async def view_bookshelf(self, event: AstrMessageEvent):
        """æŸ¥çœ‹ä¸ªäººä¹¦æ¶"""
        user_id = event.get_sender_id()
        state = self._get_user_search_state(user_id)
        parts = event.message_str.strip().split()
        
        books = self.bookshelf_manager.get_bookshelf(user_id)
        if not books:
            yield event.plain_result("ğŸ“‚ ä½ çš„ä¹¦æ¶ç©ºç©ºå¦‚ä¹Ÿï¼Œå¿«å»æœä¹¦æ·»åŠ å§ï¼")
            return

        page_size = 20
        total_pages = (len(books) + page_size - 1) // page_size
        
        # å¤„ç†åºå·æŸ¥çœ‹è¯¦æƒ…
        if len(parts) >= 2 and parts[1].isdigit():
            idx = int(parts[1])
            target = self.bookshelf_manager.get_book_by_index(user_id, idx)
            if target:
                state["last_viewed"] = target
                details = await self.source_manager.get_source(target['origin']).get_book_details(target["url"])
                if details:
                    yield event.chain_result(await self._format_book_details(details))
                return
            else:
                yield event.plain_result(f"âŒ ä¹¦æ¶ä¸­æ²¡æœ‰åºå·ä¸º {idx} çš„ä¹¦ç±ã€‚")
                return

        # å¤„ç†ç¿»é¡µ
        req_page = state.get("bookshelf_page", 1)
        if len(parts) >= 2:
            action = parts[1]
            if action in ["ä¸‹ä¸€é¡µ", "ä¸‹é¡µ"]:
                if req_page >= total_pages:
                    yield event.plain_result("ğŸ¤” å·²ç»åˆ°æœ€åä¸€é¡µäº†ã€‚")
                    return
                req_page += 1
            elif action in ["ä¸Šä¸€é¡µ", "ä¸Šé¡µ"]:
                if req_page <= 1:
                    yield event.plain_result("ğŸ¤” å·²ç»æ˜¯ç¬¬ä¸€é¡µäº†ã€‚")
                    return
                req_page -= 1
        
        state["bookshelf_page"] = req_page
        
        start_idx = (req_page - 1) * page_size
        display_list = books[start_idx : start_idx + page_size]
        
        msg = f"ğŸ“š æˆ‘çš„ä¹¦æ¶ (å…± {len(books)} æœ¬)\n\n"
        for i, b in enumerate(display_list):
            platform_tag = "[èµ·ç‚¹]" if b.get('origin') == 'qidian' else ("[åˆºçŒ¬çŒ«]" if b.get('origin') == 'ciweimao' else "[ç•ªèŒ„]")
            msg += f"{start_idx + i + 1}. {b['name']}\n    {platform_tag} ä½œè€…ï¼š{b['author']}\n"
        
        msg += f"\nğŸ’¡ `/ä¹¦æ¶ <åºå·>` æŸ¥çœ‹è¯¦æƒ…\n"
        
        # åŠ¨æ€æ„å»ºç¿»é¡µæç¤º
        page_tips = []
        if req_page > 1:
            page_tips.append("ä¸Šä¸€é¡µ")
        if req_page < total_pages:
            page_tips.append("ä¸‹ä¸€é¡µ")
            
        if page_tips:
            msg += f"ğŸ’¡ `/ä¹¦æ¶ {'/'.join(page_tips)}` ç¿»é¡µ\n"
            
        msg += f"ğŸ’¡ `/åˆ ä¹¦ <åºå·>` åˆ é™¤ä¹¦ç±"
        
        yield event.plain_result(msg.strip())

    async def _get_page_data(self, state, source_name, keyword, target_page):
        """è·å–æŒ‡å®šé¡µç æ•°æ®ï¼ˆä¼˜å…ˆè¯»å–ç¼“å­˜ï¼‰
        
        Args:
            state: ç”¨æˆ·æœç´¢çŠ¶æ€
            source_name: æ•°æ®æºåç§°ï¼ˆqidian/ciweimao/tomatoï¼‰
            keyword: æœç´¢å…³é”®è¯
            target_page: ç›®æ ‡é¡µç 
        
        Returns:
            list: è¯¥é¡µç çš„ä¹¦ç±åˆ—è¡¨
        """
        # ç¼“å­˜å‘½ä¸­ï¼šç›´æ¥è¿”å›
        if target_page in state["cached_pages"]:
            return state["cached_pages"][target_page]
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼šæ‹‰å–æ•°æ®å¹¶ç¼“å­˜
        source = self.source_manager.get_source(source_name)
        res = await source.search_book(keyword, page=target_page, return_metadata=True)
        page_data = res.get("books", [])
        state["cached_pages"][target_page] = page_data
        return page_data

    async def _common_handler(self, event: AstrMessageEvent, source_name: str, cmd_alias: str, platform_name: str):
        """å•å¹³å°æœç´¢é€šç”¨å¤„ç†é€»è¾‘
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            source_name: æ•°æ®æºåç§°ï¼ˆqidian/ciweimao/tomatoï¼‰
            cmd_alias: æŒ‡ä»¤åˆ«åï¼ˆqd/cwm/fqï¼‰
            platform_name: å¹³å°æ˜¾ç¤ºåç§°ï¼ˆèµ·ç‚¹/åˆºçŒ¬çŒ«/ç•ªèŒ„ï¼‰
        
        Yields:
            æœç´¢ç»“æœ/æç¤ºä¿¡æ¯
        """
        parts = event.message_str.strip().split()
        if len(parts) < 2:
            yield event.plain_result(f"è¯·è¾“å…¥ä¹¦åã€‚ç”¨æ³•: /{cmd_alias} <ä¹¦å>\nğŸ’¡ ç¿»é¡µ: /{cmd_alias} ä¸‹ä¸€é¡µ\nğŸ’¡ è¯¦æƒ…: /{cmd_alias} <åºå·>")
            return
        
        # è§£æç”¨æˆ·IDå’Œæ“ä½œæŒ‡ä»¤
        user_id = event.get_sender_id()
        action = parts[1]
        state = self._get_user_search_state(user_id)

        # 1. åºå·æŸ¥è¯¢ï¼šæŸ¥çœ‹å½“å‰æœç´¢ç»“æœæ± çš„ä¹¦ç±è¯¦æƒ… (e.g. /qd 1)
        if action.isdigit() and len(parts) == 2:
            seq = int(action)
            if seq < 1:
                yield event.plain_result(f"ğŸ¤” åºå· {seq} æ— æ•ˆã€‚")
                return
            
            # è®¡ç®—ç›®æ ‡é¡µç å’Œé¡µå†…ç´¢å¼•
            target_page = (seq - 1) // self.page_size + 1
            page_inner_idx = (seq - 1) % self.page_size
            
            # æ ¡éªŒæœç´¢çŠ¶æ€
            if not state["keyword"] or state["source"] != source_name:
                yield event.plain_result(f"âŒ è¯·å…ˆä½¿ç”¨ /{cmd_alias} æœç´¢ä¸€æœ¬ä¹¦ã€‚")
                return
            if target_page > state["max_pages"]:
                yield event.plain_result(f"ğŸ¤” åºå· {seq} ä¸åœ¨å½“å‰ç»“æœä¸­ã€‚")
                return
            
            # è·å–ç›®æ ‡é¡µæ•°æ®
            if target_page in state["cached_pages"]:
                page_data = state["cached_pages"][target_page]
            else:
                page_data = await self._get_page_data(state, source_name, state["keyword"], target_page)
            
            if not page_data or page_inner_idx >= len(page_data):
                yield event.plain_result(f"ğŸ¤” åºå· {seq} ä¸åœ¨å½“å‰ç»“æœä¸­ã€‚")
                return
            
            # æŸ¥è¯¢å¹¶è¿”å›ä¹¦ç±è¯¦æƒ…
            target_book = page_data[page_inner_idx]
            state["last_viewed"] = target_book # è®°å½•æœ€è¿‘æŸ¥çœ‹
            details = await self.source_manager.get_source(source_name).get_book_details(target_book["url"])
            if details:
                yield event.chain_result(await self._format_book_details(details))
            return

        # 2. ç¿»é¡µæ“ä½œ (e.g. /qd ä¸‹ä¸€é¡µ)
        if action in ["ä¸‹ä¸€é¡µ", "ä¸Šä¸€é¡µ"] and len(parts) == 2:
            if not state["keyword"] or state["source"] != source_name:
                yield event.plain_result(f"âŒ è¯·å…ˆä½¿ç”¨ /{cmd_alias} æœç´¢ä¸€æœ¬ä¹¦ã€‚")
                return
            
            next_p = state["current_page"] + (1 if action == "ä¸‹ä¸€é¡µ" else -1)
            if next_p < 1 or next_p > state["max_pages"]:
                yield event.plain_result("â¡ï¸ å·²ç»æ²¡æœ‰æ›´å¤šäº†ã€‚")
                return
            
            # è·å–ç¿»é¡µæ•°æ®
            page_data = await self._get_page_data(state, source_name, state["keyword"], next_p)
            state["current_page"] = next_p
            state["results"] = page_data
            
            # å‘é€ç¿»é¡µç»“æœ
            yield event.plain_result(self._build_search_message(
                state["keyword"], next_p, state["max_pages"], 
                page_data, cmd_alias, self.page_size, source_name
            ))
            return

        # 3. é¦–æ¬¡æœç´¢é€»è¾‘ æˆ– ç›´æ¥æŸ¥çœ‹è¯¦æƒ… (e.g. /qd è¯¡ç§˜ä¹‹ä¸» æˆ– /qd è¯¡ç§˜ä¹‹ä¸» 1)
        # è§£æç›´æ¥æŸ¥çœ‹è¯¦æƒ…ç´¢å¼•
        direct_index = None
        if len(parts) >= 3 and parts[-1].isdigit():
            try:
                direct_index = int(parts[-1])
                book_name = " ".join(parts[1:-1])
            except ValueError:
                book_name = " ".join(parts[1:])
        else:
            book_name = " ".join(parts[1:])

        yield event.plain_result(f"ğŸ” æ­£åœ¨{platform_name}æœç´¢â€œ{book_name}â€...") 
        try:
            # æ‹‰å–ç¬¬ä¸€é¡µæ•°æ®
            source = self.source_manager.get_source(source_name)
            res = await source.search_book(book_name, page=1, return_metadata=True)
            
            # æ— ç»“æœæç¤º
            if not res or not res.get("books"):
                yield event.plain_result(f"åœ¨{platform_name}æ‰¾ä¸åˆ°â€œ{book_name}â€ã€‚")
                return
            
            # å¤„ç†èµ·ç‚¹è¿”å›çš„100æ¡æ•°æ®
            first_page_data = res.get("books", [])
            if source_name == "qidian":
                # èµ·ç‚¹ä¸€æ¬¡æ€§è¿”å›100æ¡ï¼Œå…¨éƒ¨å­˜å…¥single_pool
                state["single_pool"] = first_page_data
                # è®¡ç®—æ€»é¡µæ•°ï¼ˆ10æ¡/é¡µï¼‰
                state["max_pages"] = (len(first_page_data) + self.page_size - 1) // self.page_size
                # ç¼“å­˜æ‰€æœ‰åˆ†é¡µæ•°æ®
                for i in range(state["max_pages"]):
                    start = i * self.page_size
                    end = start + self.page_size
                    state["cached_pages"][i+1] = first_page_data[start:end]
            else:
                state["max_pages"] = res.get("max_pages", 1)
                state["cached_pages"][1] = first_page_data
            
            # æ›´æ–°ç”¨æˆ·æœç´¢çŠ¶æ€
            state.update({
                "keyword": book_name, 
                "current_page": 1, 
                "source": source_name,
                "results": first_page_data[:self.page_size]  # åªå–å‰10æ¡å±•ç¤º
            })

            # å¦‚æœæ˜¯ç›´æ¥æŸ¥çœ‹è¯¦æƒ…æ¨¡å¼
            if direct_index is not None:
                if 1 <= direct_index <= len(first_page_data):
                    target_book = first_page_data[direct_index - 1]
                    state["last_viewed"] = target_book
                    details = await self.source_manager.get_source(source_name).get_book_details(target_book["url"])
                    if details:
                        yield event.chain_result(await self._format_book_details(details))
                    return
                else:
                    yield event.plain_result(f"âš ï¸ åºå· {direct_index} è¶…å‡ºç»“æœèŒƒå›´ (1-{len(first_page_data)})ï¼Œå°†æ˜¾ç¤ºæœç´¢åˆ—è¡¨ã€‚")
            
            # å‘é€ç¬¬ä¸€é¡µç»“æœ
            yield event.plain_result(self._build_search_message(
                book_name, 1, state["max_pages"], 
                first_page_data[:self.page_size], cmd_alias, self.page_size, source_name
            ))
        except Exception as e:
            logger.error(f"{platform_name} Search Error: {e}")
            yield event.plain_result("âš ï¸ æœç´¢å¤±è´¥ã€‚")

    def _build_search_message(self, keyword, current_page, max_pages, results, cmd_alias, page_size, source_name=None):
        """æ„å»ºå•å¹³å°æœç´¢ç»“æœæ¶ˆæ¯
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            current_page: å½“å‰é¡µç 
            max_pages: æ€»é¡µæ•°
            results: å½“å‰é¡µç»“æœåˆ—è¡¨
            cmd_alias: æŒ‡ä»¤åˆ«å
            page_size: æ¯é¡µæ¡æ•°
            source_name: æ•°æ®æºåç§°
        
        Returns:
            str: æ ¼å¼åŒ–åçš„æœç´¢ç»“æœæ¶ˆæ¯
        """
        # è®¡ç®—èµ·å§‹åºå·
        start_num = (current_page - 1) * page_size + 1
        
        msg = f"ä»¥ä¸‹æ˜¯ã€{keyword}ã€‘çš„ç¬¬ {current_page}/{max_pages} é¡µæœç´¢ç»“æœï¼š\n"
        for i, b in enumerate(results):
            msg += f"{start_num + i}. {b['name']}\n    ä½œè€…ï¼š{b['author']}\n"
        
        # è¡¥å……æ“ä½œæç¤º
        msg += f"\nğŸ’¡ `/{cmd_alias} <åºå·>` æŸ¥çœ‹è¯¦æƒ…\n"
        flip_tips = []
        if current_page > 1:
            flip_tips.append(f"/{cmd_alias} ä¸Šä¸€é¡µ")
        if current_page < max_pages:
            flip_tips.append(f"/{cmd_alias} ä¸‹ä¸€é¡µ")
        
        if flip_tips:
            msg += f"ğŸ’¡ ä½¿ç”¨ {' | '.join(flip_tips)} ç¿»é¡µ"
        
        return msg

    def _truncate_trial_content(self, content):
        """æˆªæ–­è¯•è¯»å†…å®¹ï¼ˆè¶…å‡ºé•¿åº¦é™åˆ¶æ·»åŠ çœç•¥å·ï¼‰
        
        Args:
            content: åŸå§‹è¯•è¯»å†…å®¹
        
        Returns:
            str: æˆªæ–­åçš„è¯•è¯»å†…å®¹
        """
        if not content:
            return ""
        cleaned_content = self._clean_text(content)
        if len(cleaned_content) <= self.trial_content_limit:
            return cleaned_content
        # æˆªæ–­å¹¶å¤„ç†ç»“å°¾æ ‡ç‚¹ï¼Œä¿è¯è¯­ä¹‰å®Œæ•´
        truncated = cleaned_content[:self.trial_content_limit].rstrip()
        if truncated[-1] in [',', '.', '!', '?', ';', ':', 'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š']:
            truncated = truncated[:-1]
        return f"{truncated}â€¦â€¦"

    async def _format_book_details(self, details):
        """æ ¼å¼åŒ–ä¹¦ç±è¯¦æƒ…æ¶ˆæ¯ï¼ˆå«å°é¢ã€åŸºç¡€ä¿¡æ¯ã€è¯•è¯»å†…å®¹ï¼‰
        
        Args:
            details: ä¹¦ç±è¯¦æƒ…å­—å…¸
        
        Returns:
            list: æ¶ˆæ¯é“¾ï¼ˆå›¾ç‰‡+æ–‡æœ¬ï¼‰
        """
        chain = []
        # å¤„ç†å°é¢å›¾ç‰‡ï¼ˆbase64ç¼–ç ï¼‰
        if details.get("cover") and details["cover"] not in ["æ— ", None]:
            cover_url = details["cover"]
            try:
                session = await self.get_session()
                # é’ˆå¯¹ç•ªèŒ„å°è¯´çš„ URL ä½¿ç”¨ encoded=Trueï¼Œé˜²æ­¢ aiohttp å¯¹å·²ç­¾åçš„ URL è¿›è¡ŒäºŒæ¬¡ç¼–ç 
                # ç•ªèŒ„å°é¢é€šå¸¸åŒ…å«ç­¾åä¿¡æ¯ï¼ŒäºŒæ¬¡ç¼–ç ä¼šå¯¼è‡´ 403
                is_tomato = "p3-novel.byteimg.com" in cover_url or "p6-novel.byteimg.com" in cover_url or "p9-novel.byteimg.com" in cover_url
                
                request_url = URL(cover_url, encoded=True) if is_tomato else cover_url
                
                async with session.get(request_url, timeout=aiohttp.ClientTimeout(total=15)) as resp:
                    if resp.status == 200:
                        image_bytes = await resp.read()
                        if image_bytes:
                            base64_str = base64.b64encode(image_bytes).decode()
                            chain.append(Comp.Image(file=f"base64://{base64_str}"))
                        else:
                            logger.warning(f"å°é¢å›¾ç‰‡æ•°æ®ä¸ºç©º: {cover_url}")
                    else:
                        logger.warning(f"å°é¢ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status}, URL: {cover_url}")
            except Exception as e:
                logger.error(f"å°é¢ä¸‹è½½å¼‚å¸¸: {type(e).__name__} - {e}, URL: {cover_url}")
        
        # æ„å»ºåŸºç¡€ä¿¡æ¯
        msg = f"---ã€{details['name']}ã€‘---\nâœï¸ ä½œè€…: {details['author']}\n"
        if details.get('category'):
            msg += f"ğŸ·ï¸ ç±»å‹: {details['category']}\n"
        
        # çŠ¶æ€ä¿¡æ¯ï¼ˆå¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²é¿å…ç±»å‹é”™è¯¯ï¼‰
        status_p = []
        if details.get('status'):
            status_p.append(str(details['status']))
        if details.get('word_count'):
            status_p.append(str(details['word_count']))
        if details.get('total_chapters'):
            status_p.append(f"å…± {details['total_chapters']}ç« ")
        if status_p:
            msg += "ğŸš¦ çŠ¶æ€: " + " | ".join(status_p) + "\n"
        
        # è¯¦ç»†æ¨¡å¼è¡¥å……ä¿¡æ¯
        if self.display_mode == "detailed":
            # æ ‡ç­¾
            if details.get('tags'):
                msg += f"ğŸ”– æ ‡ç­¾: {' / '.join(details['tags'])}\n"
            
            # è¯„åˆ†
            r, u = str(details.get('rating')), str(details.get('rating_users'))
            if r not in ["None", "0", "0.0", "æš‚æ— "]:
                if u not in ["None", "0"]:
                    msg += f"â­ è¯„åˆ†: {r} ({u}äººè¯„ä»·)\n"
                else:
                    msg += f"â­ è¯„åˆ†: {r}\n"
            
            # æ’è¡Œ
            if details.get('rank') and details['rank'] != "æœªä¸Šæ¦œ":
                msg += f"ğŸ† æ’è¡Œ: æœˆç¥¨æ¦œç¬¬ {details['rank']} å\n"
            
            # çƒ­åº¦ï¼ˆæ”¶è—/æ¨èï¼‰
            heat = []
            if details.get('collection') and str(details.get('collection')) != "0":
                heat.append(f"æ”¶è— {details['collection']}")
            if details.get('all_recommend') and str(details.get('all_recommend')) != "0":
                label = "åœ¨è¯»" if "fanqienovel.com" in details.get('url', '') else "æ¨è"
                heat.append(f"{label} {details['all_recommend']}")
            heat_str = " | ".join(heat)
            if heat_str:
                msg += f"ğŸ”¥ çƒ­åº¦: {heat_str}\n"
        
        # ç®€ä»‹
        if details.get('intro'):
            msg += f"ğŸ“ ç®€ä»‹:\n{self._clean_text(details['intro'])}\n"
        
        # æœ€è¿‘æ›´æ–°
        if self.display_mode == "detailed" and details.get('last_update'):
            upd = f"ğŸ”„ æœ€è¿‘æ›´æ–°: {details['last_update']}"
            if details.get('last_chapter'):
                upd += f" -> {details['last_chapter']}"
            msg += upd + "\n"
        
        # é“¾æ¥ï¼ˆä»…èµ·ç‚¹éœ€è¦æ›¿æ¢ç§»åŠ¨ç«¯ä¸ºPCç«¯ï¼‰
        book_url = details['url']
        if "qidian.com" in book_url:
            book_url = book_url.replace('m.qidian.com', 'www.qidian.com')
        msg += f"ğŸ”— é“¾æ¥: {book_url}\n"
        
        # è¯•è¯»å†…å®¹
        if self.enable_trial and details.get('first_chapter_title'):
            trial_content = self._truncate_trial_content(details.get('first_chapter_content', ''))
            msg += f"\nğŸ“– ã€è¯•è¯»ã€‘{details['first_chapter_title']}\n{trial_content}\n"
        
        chain.append(Comp.Plain(msg.strip()))
        return chain

    def _clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬æ ¼å¼ï¼ˆç§»é™¤HTMLæ ‡ç­¾ã€æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
        
        Returns:
            str: æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'</?p>|<br\s*/?>', '\n', text)
        text = re.sub(r'<[^>]+>', '', text)
        # æ›¿æ¢HTMLç‰¹æ®Šå­—ç¬¦
        text = text.replace("&nbsp;", " ").replace("&quot;", '"').replace("&lt;", "<").replace("&gt;", ">")
        # æ¸…ç†ç©ºè¡Œå¹¶æ ¼å¼åŒ–ç¼©è¿›
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return "ã€€ã€€" + "\nã€€ã€€".join(lines)

    async def terminate(self):
        """æ’ä»¶å¸è½½å›è°ƒ"""
        # å…³é—­æŒä¹…åŒ–ä¼šè¯
        if self._session and not self._session.closed:
            await self._session.close()
        # æ¸…ç†ç¼“å­˜ï¼Œé‡Šæ”¾å†…å­˜
        self.user_search_state.clear()
        logger.info("ç½‘æ–‡ä¿¡æ¯æœç´¢åŠ©æ‰‹æ’ä»¶å¸è½½ï¼Œç¼“å­˜å·²æ¸…ç†")