import asyncio
import aiohttp
import base64
import re
from yarl import URL
from cachetools import TTLCache
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger
import astrbot.api.message_components as Comp

from .sources import SourceManager
from .core.search_engine import MultiSearchEngine

@register("astrbot_plugin_webnovel_info", "Foolllll", "ç½‘æ–‡ä¿¡æ¯æœç´¢åŠ©æ‰‹", "0.1", "")
class WebnovelInfoPlugin(Star):
    """ç½‘æ–‡æœç´¢æ’ä»¶æ ¸å¿ƒç±»
    æ”¯æŒå¤šå¹³å°ä¹¦ç±æœç´¢ã€åˆ†é¡µã€è¯¦æƒ…æŸ¥çœ‹ã€è¯•è¯»å†…å®¹å±•ç¤º
    """
    def __init__(self, context: Context, config=None):
        super().__init__(context)
        self.source_manager = SourceManager()  # æ•°æ®æºç®¡ç†å™¨
        self.config = config or {}             # æ’ä»¶é…ç½®ï¼ˆé»˜è®¤ç©ºå­—å…¸ï¼‰
        
        # æ˜¾ç¤ºæ¨¡å¼ï¼šç®€æ´/è¯¦ç»†ï¼ˆé»˜è®¤è¯¦ç»†ï¼‰
        self.display_mode = "concise" if self.config.get("display_mode", "è¯¦ç»†") == "ç®€æ´" else "detailed"
        self.enable_trial = self.config.get("enable_trial", False)  # æ˜¯å¦å¯ç”¨è¯•è¯»åŠŸèƒ½
        self.priority_cfg = self.config.get("platform_weights", "1 2").split()  # å¹³å°æƒé‡é…ç½®
        
        # åˆå§‹åŒ–ç•ªèŒ„ API é…ç½®
        if "tomato" in self.source_manager.sources:
            self.source_manager.get_source("tomato").api_base = self.config.get("tomato_api_base", "")

        self.user_search_state = TTLCache(maxsize=1000, ttl=3600)
        
        self.trial_content_limit = 3000  # è¯•è¯»å†…å®¹é•¿åº¦é™åˆ¶ï¼ˆå­—ç¬¦æ•°ï¼‰
        self.page_size = 10  

    def _get_user_search_state(self, user_id: str):
        """è·å–/åˆå§‹åŒ–ç”¨æˆ·æœç´¢çŠ¶æ€
        
        Args:
            user_id: ç”¨æˆ·å”¯ä¸€æ ‡è¯†
        
        Returns:
            dict: ç”¨æˆ·æœç´¢çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«å…³é”®è¯ã€é¡µç ã€ç¼“å­˜ç­‰ä¿¡æ¯
        """
        if user_id not in self.user_search_state:
            self.user_search_state[user_id] = {
                "keyword": "",          # æœç´¢å…³é”®è¯
                "current_page": 1,      # å½“å‰é¡µç 
                "full_pool": [],        # å¤šå¹³å°ç»¼åˆç»“æœæ± 
                "raw_pool": [],         # åŸå§‹æœç´¢ç»“æœæ± 
                "qd_page": 1,           # èµ·ç‚¹æœç´¢é¡µç 
                "cwm_page": 1,          # åˆºçŒ¬çŒ«æœç´¢é¡µç 
                "qd_last": False,       # èµ·ç‚¹æ˜¯å¦æœ€åä¸€é¡µ
                "cwm_last": False,      # åˆºçŒ¬çŒ«æ˜¯å¦æœ€åä¸€é¡µ
                "source": "",           # å½“å‰æœç´¢æºï¼ˆmulti/qidian/ciweimaoï¼‰
                "max_pages": 1,         # æ€»é¡µæ•°
                "results": [],          # å½“å‰é¡µç»“æœ
                "single_pool": [],      # å•å¹³å°ç»“æœæ± 
                "cached_pages": {}      # é¡µç ç¼“å­˜ï¼ˆkey:é¡µç ï¼Œvalue:è¯¥é¡µæ•°æ®ï¼‰
            }
        return self.user_search_state[user_id]

    @filter.command("æœä¹¦", alias={'ss'})
    async def multi_search_handler(self, event: AstrMessageEvent):
        """å¤šå¹³å°ç»¼åˆæœç´¢å¤„ç†å‡½æ•°
        æ”¯æŒæŒ‡ä»¤ï¼š/ss <ä¹¦å> | /ss <åºå·> | /ss ä¸Šä¸€é¡µ/ä¸‹ä¸€é¡µ
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
        
        Yields:
            æœç´¢ç»“æœ/æç¤ºä¿¡æ¯
        """
        parts = event.message_str.strip().split()
        if len(parts) < 2:
            yield event.plain_result("ç”¨æ³•: /ss <ä¹¦å> æˆ– /ss <åºå·> æˆ– /ss ä¸‹ä¸€é¡µ")
            return

        # è§£æç”¨æˆ·IDå’Œæ“ä½œæŒ‡ä»¤
        user_id, action = event.get_sender_id(), parts[1]
        state = self._get_user_search_state(user_id)
        avg_threshold = 60  # ç»“æœç­›é€‰é˜ˆå€¼

        # åºå·æŸ¥è¯¢ï¼šæŸ¥çœ‹æŒ‡å®šä¹¦ç±è¯¦æƒ…
        if action.isdigit():
            idx = int(action) - 1
            if 0 <= idx < len(state["full_pool"]):
                target = state["full_pool"][idx]
                details = await self.source_manager.get_source(target['origin']).get_book_details(target["url"])
                if details:
                    yield event.chain_result(await self._format_book_details(details))
                return
            yield event.plain_result(f"ğŸ¤” åºå· {action} ä¸åœ¨å½“å‰ç»“æœä¸­ã€‚")
            return

        # ç¿»é¡µæ“ä½œ
        if action in ["ä¸‹ä¸€é¡µ", "ä¸‹é¡µ", "ä¸Šä¸€é¡µ", "ä¸Šé¡µ"]:
            if not state["keyword"]:
                yield event.plain_result("âŒ è¯·å…ˆæœç´¢ã€‚")
                return
            req_page = state["current_page"] + (1 if action in ["ä¸‹ä¸€é¡µ", "ä¸‹é¡µ"] else -1)
            if req_page < 1:
                yield event.plain_result("â¬…ï¸ å·²ç»æ˜¯ç¬¬ä¸€é¡µã€‚")
                return
            keyword = state["keyword"]
        # æ–°å…³é”®è¯æœç´¢
        else:
            keyword = " ".join(parts[1:])
            req_page = 1
            if state["keyword"] != keyword:
                yield event.plain_result(f"ğŸ” æ­£åœ¨å¤šå¹³å°æœç´¢â€œ{keyword}â€...")
                # é‡ç½®æœç´¢çŠ¶æ€
                state.update({
                    "keyword": keyword, "full_pool": [], "raw_pool": [], 
                    "qd_page": 1, "cwm_page": 1, "qd_last": False, 
                    "cwm_last": False, "source": "multi",
                    "cached_pages": {}
                })

        # è®¡ç®—ç›®æ ‡é¡µæ•°éœ€è¦çš„ç»“æœæ€»æ•°
        target_count = req_page * self.page_size
        qd_prio, cwm_prio = self.priority_cfg[0], self.priority_cfg[1]
        weights_map = {
            "qidian": MultiSearchEngine.get_weight(qd_prio), 
            "ciweimao": MultiSearchEngine.get_weight(cwm_prio)
        }

        # è¡¥å……ç»“æœæ± ç›´åˆ°æ»¡è¶³ç›®æ ‡é¡µæ•°éœ€æ±‚
        while len(state["full_pool"]) < target_count:
            _, _, current_avg = MultiSearchEngine.sift_by_average(state["raw_pool"], keyword, weights_map)
            # ç»“æœä¸è¶³æˆ–è´¨é‡ä¸è¾¾æ ‡æ—¶ï¼Œæ‹‰å–æ›´å¤šæ•°æ®
            if not state["raw_pool"] or (current_avg < avg_threshold and not (state["qd_last"] and state["cwm_last"])):
                tasks, p_map = [], []
                # èµ·ç‚¹æœç´¢ä»»åŠ¡
                if qd_prio != "0" and not state["qd_last"]:
                    tasks.append(self.source_manager.get_source("qidian").search_book(keyword, page=state["qd_page"], return_metadata=True))
                    p_map.append("qidian")
                # åˆºçŒ¬çŒ«æœç´¢ä»»åŠ¡
                if cwm_prio != "0" and not state["cwm_last"]:
                    tasks.append(self.source_manager.get_source("ciweimao").search_book(keyword, page=state["cwm_page"], return_metadata=True))
                    p_map.append("ciweimao")
                if not tasks:
                    break
                # å¹¶å‘æ‰§è¡Œæœç´¢ä»»åŠ¡
                results = await asyncio.gather(*tasks)
                for i, r in enumerate(results):
                    if not r:
                        continue
                    books = r.get('books', [])
                    # æ›´æ–°å¹³å°é¡µç å’Œæ˜¯å¦æœ€åä¸€é¡µçŠ¶æ€
                    if p_map[i] == "qidian":
                        state["qd_page"] += 1
                        state["qd_last"] = r.get('is_last', False)
                    else:
                        state["cwm_page"] += 1
                        state["cwm_last"] = r.get('is_last', False)
                    state["raw_pool"].extend(books)
                continue

            # ç­›é€‰é«˜è´¨é‡ç»“æœå¹¶äº¤å‰æ’åº
            good_batch, remains, _ = MultiSearchEngine.sift_by_average(state["raw_pool"], keyword, weights_map)
            if good_batch:
                interleaved = MultiSearchEngine.interleave_results(good_batch, qd_prio, cwm_prio)
                state["full_pool"].extend(interleaved)
                state["raw_pool"] = remains
            else:
                # æ— é«˜è´¨é‡ç»“æœæ—¶ï¼Œè¡¥å……å‰©ä½™åŸå§‹ç»“æœ
                if state["qd_last"] and state["cwm_last"]:
                    state["full_pool"].extend(state["raw_pool"])
                    state["raw_pool"] = []
                    break
                state["raw_pool"] = []

        # è®¡ç®—å½“å‰é¡µå±•ç¤ºçš„ç»“æœèŒƒå›´
        start_idx = (req_page - 1) * self.page_size
        display_list = state["full_pool"][start_idx: start_idx + self.page_size]
        state["current_page"] = req_page

        # æ— ç»“æœæç¤º
        if not display_list:
            yield event.plain_result(f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°åŒ¹é…â€œ{keyword}â€çš„é«˜è´¨é‡ç»“æœã€‚")
            return

        # 1. æ£€æŸ¥æ˜¯å¦è¿˜èƒ½æ‹‰å–æ›´å¤šæ•°æ®ï¼ˆèµ·ç‚¹/åˆºçŒ¬çŒ«æœªåˆ°æœ€åä¸€é¡µï¼‰
        can_load_more = False
        if not state["qd_last"] or not state["cwm_last"]:
            can_load_more = True
        
        # 2. è®¡ç®—å½“å‰æ€»é¡µæ•°ï¼ˆå·²åŠ è½½æ•°æ®ï¼‰
        current_total_pages = (len(state["full_pool"]) + self.page_size - 1) // self.page_size
        # 3. åˆ¤æ–­æ˜¯å¦æœ‰ä¸‹ä¸€é¡µï¼ˆå·²åŠ è½½å¤Ÿä¸‹ä¸€é¡µ æˆ– è¿˜èƒ½åŠ è½½æ›´å¤šæ•°æ®ï¼‰
        has_next_page = False
        if req_page < current_total_pages:
            has_next_page = True
        elif can_load_more and (req_page + 1) * self.page_size > len(state["full_pool"]):
            has_next_page = True
        
        # 4. æ„å»ºæ¶ˆæ¯ï¼ˆæ˜¾ç¤ºæ€»é¡µæ•°ï¼‰
        can_load_more = not state["qd_last"] or not state["cwm_last"]
        current_total_pages = (len(state["full_pool"]) + self.page_size - 1) // self.page_size
        
        if can_load_more:
            msg = f"ä»¥ä¸‹æ˜¯ã€{keyword}ã€‘çš„ç¬¬ {req_page} é¡µç»¼åˆæœç´¢ç»“æœï¼š\n"  # æœ‰æ›´å¤šâ†’åªæ˜¾ç¤ºå½“å‰é¡µ
        else:
            msg = f"ä»¥ä¸‹æ˜¯ã€{keyword}ã€‘çš„ç¬¬ {req_page}/{current_total_pages} é¡µç»¼åˆæœç´¢ç»“æœï¼š\n"  # æ— æ›´å¤šâ†’æ˜¾ç¤ºæ€»é¡µæ•°
        for i, b in enumerate(display_list):
            platform_tag = "[èµ·ç‚¹]" if b.get('origin') == 'qidian' else ("[åˆºçŒ¬çŒ«]" if b.get('origin') == 'ciweimao' else "[ç•ªèŒ„]")
            msg += f"{start_idx + i + 1}. {b['name']}\n    {platform_tag} ä½œè€…ï¼š{b['author']}\n"
        
        # 5. æ„å»ºç¿»é¡µæç¤º
        page_tips = []
        page_tips.append(f"/ss ä¸Šä¸€é¡µ") if req_page > 1 else None
        page_tips.append(f"/ss ä¸‹ä¸€é¡µ") if has_next_page else None
        
        logger.info(f"ç”¨æˆ· {user_id} æœç´¢ã€{keyword}ã€‘ç¬¬ {req_page} é¡µç»“æœï¼Œå½“å‰æ± ä¸­å…±æœ‰ {len(state['full_pool'])} æ¡ç»“æœï¼Œå¯åŠ è½½æ›´å¤šï¼š{can_load_more}ã€‚")
        
        # 6. è¡¥å……æ“ä½œæç¤º
        msg += f"\nğŸ’¡ `/ss <åºå·>` æŸ¥çœ‹è¯¦æƒ…\n"
        if page_tips:
            msg += f"ğŸ’¡ ä½¿ç”¨ {' | '.join(page_tips)} ç¿»é¡µ"
        else:
            if req_page == 1 and len(state["full_pool"]) <= self.page_size and not can_load_more:
                msg += "ğŸ’¡ å½“å‰å·²æ˜¯å…¨éƒ¨ç»“æœï¼Œæ— æ›´å¤šå†…å®¹"
            elif req_page > 1 and not has_next_page and not can_load_more:
                msg += "ğŸ’¡ å½“å‰å·²æ˜¯æœ€åä¸€é¡µï¼Œæ— æ›´å¤šå†…å®¹"
        
        yield event.plain_result(msg)

    @filter.command("èµ·ç‚¹", alias={'qd'})
    async def qidian_handler(self, event: AstrMessageEvent):
        """èµ·ç‚¹ä¸­æ–‡ç½‘ä¸“å±æœç´¢"""
        async for res in self._common_handler(event, "qidian", "qd", "èµ·ç‚¹"):
            yield res

    @filter.command("åˆºçŒ¬çŒ«", alias={'cwm'})
    async def ciweimao_handler(self, event: AstrMessageEvent):
        """åˆºçŒ¬çŒ«ä¸“å±æœç´¢"""
        async for res in self._common_handler(event, "ciweimao", "cwm", "åˆºçŒ¬çŒ«"):
            yield res

    @filter.command("ç•ªèŒ„", alias={'fq'})
    async def tomato_handler(self, event: AstrMessageEvent):
        """ç•ªèŒ„å°è¯´ä¸“å±æœç´¢"""
        if not self.config.get("tomato_api_base"):
            yield event.plain_result("âŒ æœªé…ç½®ç•ªèŒ„ API åŸºç¡€åœ°å€ï¼Œè¯·åœ¨é…ç½®ä¸­å¡«å†™ã€‚")
            return
        async for res in self._common_handler(event, "tomato", "fq", "ç•ªèŒ„"):
            yield res

    async def _get_page_data(self, state, source_name, keyword, target_page):
        """è·å–æŒ‡å®šé¡µç æ•°æ®ï¼ˆä¼˜å…ˆè¯»å–ç¼“å­˜ï¼‰
        
        Args:
            state: ç”¨æˆ·æœç´¢çŠ¶æ€
            source_name: æ•°æ®æºåç§°ï¼ˆqidian/ciweimao/tomatoï¼‰
            keyword: æœç´¢å…³é”®è¯
            target_page: ç›®æ ‡é¡µç 
        
        Returns:
            list: è¯¥é¡µç çš„ä¹¦ç±åˆ—è¡¨
        """
        # ç¼“å­˜å‘½ä¸­ï¼šç›´æ¥è¿”å›
        if target_page in state["cached_pages"]:
            return state["cached_pages"][target_page]
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼šæ‹‰å–æ•°æ®å¹¶ç¼“å­˜
        source = self.source_manager.get_source(source_name)
        res = await source.search_book(keyword, page=target_page, return_metadata=True)
        page_data = res.get("books", [])
        state["cached_pages"][target_page] = page_data
        return page_data

    async def _common_handler(self, event: AstrMessageEvent, source_name: str, cmd_alias: str, platform_name: str):
        """å•å¹³å°æœç´¢é€šç”¨å¤„ç†é€»è¾‘
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            source_name: æ•°æ®æºåç§°ï¼ˆqidian/ciweimao/tomatoï¼‰
            cmd_alias: æŒ‡ä»¤åˆ«åï¼ˆqd/cwm/fqï¼‰
            platform_name: å¹³å°æ˜¾ç¤ºåç§°ï¼ˆèµ·ç‚¹/åˆºçŒ¬çŒ«/ç•ªèŒ„ï¼‰
        
        Yields:
            æœç´¢ç»“æœ/æç¤ºä¿¡æ¯
        """
        parts = event.message_str.strip().split()
        if len(parts) < 2:
            yield event.plain_result(f"è¯·è¾“å…¥ä¹¦åã€‚ç”¨æ³•: /{cmd_alias} <ä¹¦å>\nğŸ’¡ ç¿»é¡µ: /{cmd_alias} ä¸‹ä¸€é¡µ\nğŸ’¡ è¯¦æƒ…: /{cmd_alias} <åºå·>")
            return
        
        # è§£æç”¨æˆ·IDå’Œæ“ä½œæŒ‡ä»¤
        user_id = event.get_sender_id()
        action = parts[1]
        state = self._get_user_search_state(user_id)

        # åºå·æŸ¥è¯¢ï¼šæŸ¥çœ‹ä¹¦ç±è¯¦æƒ…
        if action.isdigit():
            seq = int(action)
            if seq < 1:
                yield event.plain_result(f"ğŸ¤” åºå· {seq} æ— æ•ˆã€‚")
                return
            
            # è®¡ç®—ç›®æ ‡é¡µç å’Œé¡µå†…ç´¢å¼•
            target_page = (seq - 1) // self.page_size + 1
            page_inner_idx = (seq - 1) % self.page_size
            
            # æ ¡éªŒæœç´¢çŠ¶æ€
            if not state["keyword"] or state["source"] != source_name:
                yield event.plain_result(f"âŒ è¯·å…ˆä½¿ç”¨ /{cmd_alias} æœç´¢ä¸€æœ¬ä¹¦ã€‚")
                return
            if target_page > state["max_pages"]:
                yield event.plain_result(f"ğŸ¤” åºå· {seq} ä¸åœ¨å½“å‰ç»“æœä¸­ã€‚")
                return
            
            # è·å–ç›®æ ‡é¡µæ•°æ®
            if target_page in state["cached_pages"]:
                page_data = state["cached_pages"][target_page]
            else:
                page_data = await self._get_page_data(state, source_name, state["keyword"], target_page)
            
            if not page_data or page_inner_idx >= len(page_data):
                yield event.plain_result(f"ğŸ¤” åºå· {seq} ä¸åœ¨å½“å‰ç»“æœä¸­ã€‚")
                return
            
            # æŸ¥è¯¢å¹¶è¿”å›ä¹¦ç±è¯¦æƒ…
            target_book = page_data[page_inner_idx]
            details = await self.source_manager.get_source(source_name).get_book_details(target_book["url"])
            if details:
                yield event.chain_result(await self._format_book_details(details))
            return

        # ç¿»é¡µæ“ä½œ
        if action in ["ä¸‹ä¸€é¡µ", "ä¸Šä¸€é¡µ"]:
            if not state["keyword"] or state["source"] != source_name:
                yield event.plain_result(f"âŒ è¯·å…ˆä½¿ç”¨ /{cmd_alias} æœç´¢ä¸€æœ¬ä¹¦ã€‚")
                return
            
            next_p = state["current_page"] + (1 if action == "ä¸‹ä¸€é¡µ" else -1)
            if next_p < 1 or next_p > state["max_pages"]:
                yield event.plain_result("â¡ï¸ å·²ç»æ²¡æœ‰æ›´å¤šäº†ã€‚")
                return
            
            # è·å–ç¿»é¡µæ•°æ®
            page_data = await self._get_page_data(state, source_name, state["keyword"], next_p)
            state["current_page"] = next_p
            state["results"] = page_data
            
            # å‘é€ç¿»é¡µç»“æœ
            yield event.plain_result(self._build_search_message(
                state["keyword"], next_p, state["max_pages"], 
                page_data, cmd_alias, self.page_size, source_name
            ))
            return

        # é¦–æ¬¡æœç´¢é€»è¾‘
        book_name = " ".join(parts[1:])
        yield event.plain_result(f"ğŸ” æ­£åœ¨{platform_name}æœç´¢â€œ{book_name}â€...") 
        try:
            # æ‹‰å–ç¬¬ä¸€é¡µæ•°æ®
            source = self.source_manager.get_source(source_name)
            res = await source.search_book(book_name, page=1, return_metadata=True)
            
            # æ— ç»“æœæç¤º
            if not res or not res.get("books"):
                yield event.plain_result(f"åœ¨{platform_name}æ‰¾ä¸åˆ°â€œ{book_name}â€ã€‚")
                return
            
            # å¤„ç†èµ·ç‚¹è¿”å›çš„100æ¡æ•°æ®
            first_page_data = res.get("books", [])
            if source_name == "qidian":
                # èµ·ç‚¹ä¸€æ¬¡æ€§è¿”å›100æ¡ï¼Œå…¨éƒ¨å­˜å…¥single_pool
                state["single_pool"] = first_page_data
                # è®¡ç®—æ€»é¡µæ•°ï¼ˆ10æ¡/é¡µï¼‰
                state["max_pages"] = (len(first_page_data) + self.page_size - 1) // self.page_size
                # ç¼“å­˜æ‰€æœ‰åˆ†é¡µæ•°æ®
                for i in range(state["max_pages"]):
                    start = i * self.page_size
                    end = start + self.page_size
                    state["cached_pages"][i+1] = first_page_data[start:end]
            else:
                state["max_pages"] = res.get("max_pages", 1)
                state["cached_pages"][1] = first_page_data
            
            # æ›´æ–°ç”¨æˆ·æœç´¢çŠ¶æ€
            state.update({
                "keyword": book_name, 
                "current_page": 1, 
                "source": source_name,
                "results": first_page_data[:self.page_size]  # åªå–å‰10æ¡å±•ç¤º
            })
            
            # å‘é€ç¬¬ä¸€é¡µç»“æœ
            yield event.plain_result(self._build_search_message(
                book_name, 1, state["max_pages"], 
                first_page_data[:self.page_size], cmd_alias, self.page_size, source_name
            ))
        except Exception as e:
            logger.error(f"{platform_name} Search Error: {e}")
            yield event.plain_result("âš ï¸ æœç´¢å¤±è´¥ã€‚")

    def _build_search_message(self, keyword, current_page, max_pages, results, cmd_alias, page_size, source_name=None):
        """æ„å»ºå•å¹³å°æœç´¢ç»“æœæ¶ˆæ¯
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            current_page: å½“å‰é¡µç 
            max_pages: æ€»é¡µæ•°
            results: å½“å‰é¡µç»“æœåˆ—è¡¨
            cmd_alias: æŒ‡ä»¤åˆ«å
            page_size: æ¯é¡µæ¡æ•°
            source_name: æ•°æ®æºåç§°
        
        Returns:
            str: æ ¼å¼åŒ–åçš„æœç´¢ç»“æœæ¶ˆæ¯
        """
        # è®¡ç®—èµ·å§‹åºå·
        start_num = (current_page - 1) * page_size + 1
        
        msg = f"ä»¥ä¸‹æ˜¯ã€{keyword}ã€‘çš„ç¬¬ {current_page}/{max_pages} é¡µæœç´¢ç»“æœï¼š\n"
        for i, b in enumerate(results):
            msg += f"{start_num + i}. {b['name']}\n    ä½œè€…ï¼š{b['author']}\n"
        
        # è¡¥å……æ“ä½œæç¤º
        msg += f"\nğŸ’¡ `/{cmd_alias} <åºå·>` æŸ¥çœ‹è¯¦æƒ…\n"
        flip_tips = []
        if current_page > 1:
            flip_tips.append(f"/{cmd_alias} ä¸Šä¸€é¡µ")
        if current_page < max_pages:
            flip_tips.append(f"/{cmd_alias} ä¸‹ä¸€é¡µ")
        
        if flip_tips:
            msg += f"ğŸ’¡ ä½¿ç”¨ {' | '.join(flip_tips)} ç¿»é¡µ"
        
        return msg

    def _truncate_trial_content(self, content):
        """æˆªæ–­è¯•è¯»å†…å®¹ï¼ˆè¶…å‡ºé•¿åº¦é™åˆ¶æ·»åŠ çœç•¥å·ï¼‰
        
        Args:
            content: åŸå§‹è¯•è¯»å†…å®¹
        
        Returns:
            str: æˆªæ–­åçš„è¯•è¯»å†…å®¹
        """
        if not content:
            return ""
        cleaned_content = self._clean_text(content)
        if len(cleaned_content) <= self.trial_content_limit:
            return cleaned_content
        # æˆªæ–­å¹¶å¤„ç†ç»“å°¾æ ‡ç‚¹ï¼Œä¿è¯è¯­ä¹‰å®Œæ•´
        truncated = cleaned_content[:self.trial_content_limit].rstrip()
        if truncated[-1] in [',', '.', '!', '?', ';', ':', 'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š']:
            truncated = truncated[:-1]
        return f"{truncated}â€¦â€¦"

    async def _format_book_details(self, details):
        """æ ¼å¼åŒ–ä¹¦ç±è¯¦æƒ…æ¶ˆæ¯ï¼ˆå«å°é¢ã€åŸºç¡€ä¿¡æ¯ã€è¯•è¯»å†…å®¹ï¼‰
        
        Args:
            details: ä¹¦ç±è¯¦æƒ…å­—å…¸
        
        Returns:
            list: æ¶ˆæ¯é“¾ï¼ˆå›¾ç‰‡+æ–‡æœ¬ï¼‰
        """
        chain = []
        # å¤„ç†å°é¢å›¾ç‰‡ï¼ˆbase64ç¼–ç ï¼‰
        if details.get("cover") and details["cover"] not in ["æ— ", None]:
            cover_url = details["cover"]
            try:
                # å…³é”®ï¼šä½¿ç”¨ yarl.URL(encoded=True) é˜²æ­¢ aiohttp è‡ªåŠ¨å¯¹å·²ç­¾åçš„ URL è¿›è¡ŒäºŒæ¬¡ç¼–ç å¯¼è‡´ 403
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                }
                async with aiohttp.ClientSession(headers=headers) as session:
                    async with session.get(URL(cover_url, encoded=True), timeout=10) as resp:
                        if resp.status == 200:
                            image_bytes = await resp.read()
                            chain.append(Comp.Image(file=f"base64://{base64.b64encode(image_bytes).decode()}"))
                        else:
                            logger.warning(f"å°é¢ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status}, URL: {cover_url}")
            except Exception as e:
                logger.error(f"å°é¢ä¸‹è½½å¼‚å¸¸: {e}")
                pass
        
        # æ„å»ºåŸºç¡€ä¿¡æ¯
        msg = f"---ã€{details['name']}ã€‘---\nâœï¸ ä½œè€…: {details['author']}\n"
        if details.get('category'):
            msg += f"ğŸ·ï¸ ç±»å‹: {details['category']}\n"
        
        # çŠ¶æ€ä¿¡æ¯ï¼ˆå¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²é¿å…ç±»å‹é”™è¯¯ï¼‰
        status_p = []
        if details.get('status'):
            status_p.append(str(details['status']))
        if details.get('word_count'):
            status_p.append(str(details['word_count']))
        if details.get('total_chapters'):
            status_p.append(f"å…± {details['total_chapters']}ç« ")
        if status_p:
            msg += "ğŸš¦ çŠ¶æ€: " + " | ".join(status_p) + "\n"
        
        # è¯¦ç»†æ¨¡å¼è¡¥å……ä¿¡æ¯
        if self.display_mode == "detailed":
            # æ ‡ç­¾
            if details.get('tags'):
                msg += f"ğŸ”– æ ‡ç­¾: {' / '.join(details['tags'])}\n"
            
            # è¯„åˆ†
            r, u = str(details.get('rating')), str(details.get('rating_users'))
            if r not in ["None", "0", "0.0", "æš‚æ— "]:
                if u not in ["None", "0"]:
                    msg += f"â­ è¯„åˆ†: {r} ({u}äººè¯„ä»·)\n"
                else:
                    msg += f"â­ è¯„åˆ†: {r}\n"
            
            # æ’è¡Œ
            if details.get('rank') and details['rank'] != "æœªä¸Šæ¦œ":
                msg += f"ğŸ† æ’è¡Œ: æœˆç¥¨æ¦œç¬¬ {details['rank']} å\n"
            
            # çƒ­åº¦ï¼ˆæ”¶è—/æ¨èï¼‰
            heat = []
            if details.get('collection') and str(details.get('collection')) != "0":
                heat.append(f"æ”¶è— {details['collection']}")
            if details.get('all_recommend') and str(details.get('all_recommend')) != "0":
                label = "åœ¨è¯»" if "fanqienovel.com" in details.get('url', '') else "æ¨è"
                heat.append(f"{label} {details['all_recommend']}")
            heat_str = " | ".join(heat)
            if heat_str:
                msg += f"ğŸ”¥ çƒ­åº¦: {heat_str}\n"
        
        # ç®€ä»‹
        if details.get('intro'):
            msg += f"ğŸ“ ç®€ä»‹:\n{self._clean_text(details['intro'])}\n"
        
        # æœ€è¿‘æ›´æ–°
        if self.display_mode == "detailed" and details.get('last_update'):
            upd = f"ğŸ”„ æœ€è¿‘æ›´æ–°: {details['last_update']}"
            if details.get('last_chapter'):
                upd += f" -> {details['last_chapter']}"
            msg += upd + "\n"
        
        # é“¾æ¥ï¼ˆä»…èµ·ç‚¹éœ€è¦æ›¿æ¢ç§»åŠ¨ç«¯ä¸ºPCç«¯ï¼‰
        book_url = details['url']
        if "qidian.com" in book_url:
            book_url = book_url.replace('m.qidian.com', 'www.qidian.com')
        msg += f"ğŸ”— é“¾æ¥: {book_url}\n"
        
        # è¯•è¯»å†…å®¹
        if self.enable_trial and details.get('first_chapter_title'):
            trial_content = self._truncate_trial_content(details.get('first_chapter_content', ''))
            msg += f"\nğŸ“– ã€è¯•è¯»ã€‘{details['first_chapter_title']}\n{trial_content}\n"
        
        chain.append(Comp.Plain(msg.strip()))
        return chain

    def _clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬æ ¼å¼ï¼ˆç§»é™¤HTMLæ ‡ç­¾ã€æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
        
        Returns:
            str: æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'</?p>|<br\s*/?>', '\n', text)
        text = re.sub(r'<[^>]+>', '', text)
        # æ›¿æ¢HTMLç‰¹æ®Šå­—ç¬¦
        text = text.replace("&nbsp;", " ").replace("&quot;", '"').replace("&lt;", "<").replace("&gt;", ">")
        # æ¸…ç†ç©ºè¡Œå¹¶æ ¼å¼åŒ–ç¼©è¿›
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return "ã€€ã€€" + "\nã€€ã€€".join(lines)

    async def terminate(self):
        """æ’ä»¶å¸è½½å›è°ƒ"""
        # æ¸…ç†ç¼“å­˜ï¼Œé‡Šæ”¾å†…å­˜
        self.user_search_state.clear()
        logger.info("ç½‘æ–‡ä¿¡æ¯æœç´¢åŠ©æ‰‹æ’ä»¶å¸è½½ï¼Œç¼“å­˜å·²æ¸…ç†")